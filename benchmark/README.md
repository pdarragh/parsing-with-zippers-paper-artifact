# Benchmarking Parsing with Zippers

This directory contains a copy of the code used to benchmark Parsing with
Zippers, the results of which appear in the associated paper.

The short version of this is: install the language runtimes and library
dependencies as outline

## Requirements

There are three branches of dependencies in this project:

1. The Python code used for tokenization and general scripting.
2. The OCaml code used for parsing and benchmarking.
3. The PDF generated that contains the graphs and calculations used in the
   paper.

### Python Requirements

To install Python 3.7.3 specifically, we recommend installing from source
according to the instructions on the Python language website. However, this code
should work with any 3.7.x version of Python where x is greater than 3, so it is
likely that simply using your package manager to install `python-3.7` will be
sufficient. Note that we use `pip3` for installing additional dependencies, but
there are alternative methods if you prefer to go that route.

The only non-standard package used directly is `parso`, which can be installed
by doing `pip3 install parso==0.4.0`. Parso provides a better tokenization
experience than the standard library's `tokenize` module, specifically because
it supports tokenizing code written in versions of Python other than the
installed version.

### OCaml Requirements

To install OCaml 4.05.0, first install Opam using your package manger (e.g.,
`apt install opam`). Then, use Opam to install the correct version by doing
`opam switch create pwz ocaml-system.4.05.0`.

In addition, the following external dependencies are needed. They can all be
installed by performing `opam install core.v0.11.3 core_bench.v0.11.0 menhir.20200211 dypgen.20120619-1 -y`.

- [Core](https://ocaml.janestreet.com/ocaml-core/latest/doc/core/index.html),
  v 0.11.3
    - Core is an improved standard library for OCaml, managed by Jane Street. We
      mostly use this for managing command-line options and association lists.
- [Core_bench](https://ocaml.janestreet.com/ocaml-core/latest/doc/core_bench/Core_bench/index.html),
  v 0.11.0
    - A micro-benchmarking library for OCaml, managed by Jane Street. This is
      used for all benchmarking.
- [Menhir](http://gallium.inria.fr/~fpottier/menhir/), v 20200211
    - LR(1) parser generator for OCaml. We use this to compare performance.
- [Dypgen](http://dypgen.free.fr), v 20120619-1
    - GLR parser generator for OCaml. We use this to compare performance.

### PDF Requirements

The PDF generation requires LuaTeX. Any modern version of LuaTeX with a full
TeXLive installation should be sufficient. We used version 1.10.0.

## Running

All builds and processing can be performed using the top-level `Makefile`.
Targets and command-line parameters are explained further below, but a brief
overview is given here.

The short version is:

```
$ make prepare
$ make benchmark
$ make post-process
```

But we explain these steps more below.

### Running, Explained

To generate and compile the various parsers, do:

```
$ make prepare
```

This will extract the suggested `.py` files from the 3.4.3 Python source code,
lex the resulting files, generate the compiler code, and build the project.
This will not run the benchmarks yet. We recommend running this target
separately so that compilation issues can more easily be addressed, if any
arise.

After `make prepare` completes successfully, you can run the benchmark suite. To
do this, run:

```
$ make benchmark
```

Note that this is a **long** process. Benchmarking on a computer with a
dedicated CPU took almost 36 hours. The [Reducing Benchmark
Time](#reducing-benchmark-time) section below explains some methods of reducing
the time taken.

However, the benchmark process is smart. You can cancel the running benchmarks
(by the typical `CTRL+c` / `<C-c>`) and later resume the benchmarks (via `make
benchmark`) and it will pick up right where you left off.

After benchmarking completes, the paper's graphs and calculations can be
generated by doing:

```
$ make post-process
```

This will produce a PDF in `$OUT_FILE_DIR` (`./out/results.pdf` by default)
containing the graphs and calculations used in the paper.

To run all of these targets in series automatically, you can do:

```
$ make all
```

### Producing the Data from the Paper

We have also included the original data used to produce the paper for
comparison. Graphs can be produced from this data using:

```
$ make paper-graphs
```

### Targets

The Makefile accepts many targets. However, only a few of them are meant for
general use. These are:

  * `default` (the target that runs if you simply do `make` with no arguments)
  * `all` (runs everything without stopping)
  * `prepare` (prepares the directory for benchmarking, but does not start the
    benchmarking process)
  * `benchmark` (after preparing, runs the benchmarking code)
  * `post-process` (after benchmarking, produces the graphs and calculations
    used in the paper)

Below, we give the full list of supported targets (each used as `make <target>`)
and the parameters they can interact with. Parameters are explained in the next
section. If a target's summary says that it runs another target, the second
target's parameters can also be used with the first target.

| Target Name          | Summary                                                                                               | Parameters Used                                                                       |
|----------------------|-------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------|
| `default`            | Runs `prepare`.                                                                                       |                                                                                       |
| `all`                | Runs `clean-all`, `prepare`, `benchmark`.                                                             |                                                                                       |
| `clean`              | Runs `clean-compile`.                                                                                 |                                                                                       |
| `clean-all`          | Runs `clean-prepare`, `clean-benchmark`, and `clean-post-process`.                                    |                                                                                       |
| `clean-prepare`      | Runs `clean-extract`, `clean-lex`, and `clean-generate`.                                              |                                                                                       |
| `clean-post-process` | Runs `clean-out`.                                                                                     |                                                                                       |
| `clean-extract`      | Deletes all `.py` files in `$PY_FILE_DIR`.                                                            | `$PY_FILE_DIR`                                                                        |
| `clean-lex`          | Deletes all `.lex` files in `$LEX_FILE_DIR`.                                                          | `$LEX_FILE_DIR`                                                                       |
| `clean-generate`     | Deletes all files in `$GEN_FILE_DIR`.                                                                 | `$GEN_FILE_DIR`                                                                       |
| `clean-compile`      | Runs the `clean` target in `$GEN_FILE_DIR/Makefile`.                                                  | `$GEN_FILE_DIR`                                                                       |
| `clean-benchmark`    | Deletes all `.bench` files in `$BENCH_FILE_DIR`.                                                      | `$BENCH_FILE_DIR`                                                                     |
| `clean-graphs`       | Deletes unneeded files in `$GRAPHS_FILE_DIR`.                                                         | `$GRAPHS_FILE_DIR`                                                                    |
| `clean-out`          | Deletes all files in `$OUT_FILE_DIR`.                                                                 | `$OUT_FILE_DIR`                                                                       |
| `clean-parse`        | Deletes all files in `$AST_FILE_DIR`.                                                                 | `$AST_FILE_DIR`                                                                       |
| `prepare`            | Runs `extract`, `lex`, `generate`, and `compile`.                                                     |                                                                                       |
| `extract`            | Extracts the necessary files from the Python source code tarball `$TGZ_FILE` into `$PY_FILE_DIR`.     | `$TGZ_FILE`, `$PY_FILE_DIR`                                                           |
| `lex`                | Lexes all `.py` files found in `$PY_FILE_DIR` and outputs the results to `$LEX_FILE_DIR`.             | `$PY_FILE_DIR`, `$LEX_FILE_DIR`, `PYTHON`                                             |
| `generate`           | Generates all the files needed for compiling the executables. Code will be placed in `$GEN_FILE_DIR`. | `$GEN_FILE_DIR`, `$PYTHON`, `$GRAMMAR_FILE`.                                          |
| `compile`            | Compiles the executables `$BENCH_OUT` (for benchmarking) and `$PARSE_OUT` (for parsing).              | `$BENCH_OUT`, `$PARSE_OUT`                                                            |
| `benchmark`          | Runs benchmarks over all `.lex` files found in `$LEX_FILE_DIR`.                                       | `$LEX_FILE_DIR`, `$BENCH_FILE_DIR`, `$BENCH_OUT`                                      |
| `post-process`       | Runs `collate` and `graphs`.                                                                          |                                                                                       |
| `collate`            | Collates the results of `benchmark` into a single `.csv` file, `$COLLATED_RESULTS_FILE`.              | `$COLLATED_RESULTS_FILE`                                                              |
| `graphs`             | Produces a PDF of the graphs used in the paper.                                                       | `GRAPHS_FILE_DIR`, `$OUT_FILE_DIR`, `$COLLATED_RESULTS_FILE`, `$RECURSIVE_CALLS_FILE` |
| `paper-graphs`       | Produces a PDF like `graphs`, but using the data we used for producing the paper.                     |                                                                                       |
| `parse`              | Parses all `.lex` files found in `$LEX_FILE_DIR` into `.ast` files placed in `$AST_FILE_DIR`.         | `$LEX_FILE_DIR`, `$AST_FILE_DIR`, `$PARSE_OUT`                                        |
| `verify`             | Verifies all existing `.ast` files against the Menhir baseline.                                       | `$AST_FILE_DIR`                                                                       |
| `compile-profile`    | Like `compile`, but includes instrumentation for profiling.                                           | (same as `compile`)                                                                   |

### Parameters

Parameters can be given in the usual Makefile fashion (i.e., `PARAM=value make
<target>`). Each supported parameter is explained below, along with a summary of
the default value. Note that all parameters have default values, so you do not
need to specify any parameters to run the targets. Relative paths are given
relative to the root directory of this repository (which should be where this
README is located).

| Parameter Name          | Summary                                                                         | Default Value                                        |
|-------------------------|---------------------------------------------------------------------------------|------------------------------------------------------|
| `PYTHON`                | Path to Python 3.7+ executable.                                                 | `python3`                                            |
| `TGZ_FILE`              | Path to the Python source code tarball.                                         | `./Python-3.4.3.tgz`                                 |
| `GRAMMAR_FILE`          | Path to Python grammar used for parser generation.                              | `./pwz_bench/utility/transformed-python-3.4.grammar` |
| `START_SYMBOLS`         | Space-separated list of start symbols in `$GRAMMAR_FILE`.                       | `single_input file_input eval_input`                 |
| `GEN_FILE_DIR`          | Directory to output generated code.                                             | `./gen/`                                             |
| `PY_FILE_DIR`           | Directory where base `.py` files are located/should be extracted to.            | `./pys/`                                             |
| `LEX_FILE_DIR`          | Directory where lexed `.lex` files should be located.                           | `./lexes/`                                           |
| `AST_FILE_DIR`          | Directory where parsed `.ast` output files should be saved.                     | `./parses/`                                          |
| `BENCH_FILE_DIR`        | Directory where benchmarking `.bench` files should be saved.                    | `./bench/`                                           |
| `GRAPHS_FILE_DIR`       | Directory where temporary graphing-related files should be saved.               | `./graphs/`                                          |
| `OUT_FILE_DIR`          | Directory to output graphs and calculations used in the paper.                  | `./out/`                                             |
| `BENCH_OUT`             | Name of the benchmarking executable.                                            | `$GEN_FILE_DIR/pwz_bench`                           |
| `PARSE_OUT`             | Name of the parsing executable.                                                 | `$GEN_FILE_DIR/pwz_parse`                           |
| `COLLATED_RESULTS_FILE` | Name of the file output by `collate` and used by `graphs` for producing graphs. | `$OUT_FILE_DIR/collated-results.csv`                 |
| `RECURSIVE_CALLS_FILE`  | Name of the file for measuring recursive calls, used by `graphs`.               | `$GRAPHS_FILE_DIR/recursive-calls.csv`               |
| `TIMEOUT`               | Maximum length of per-execution timeout during benchmarking in seconds.         | -1 (no maximum timeout)                              |
| `QUOTA_FACTOR`          | The factor by which to increase the quota during subsequent runs.               | 3                                                    |
| `MAX_QUOTA`             | The maximum allowable quota value. Benchmarks that go over this fail.           | 1000                                                 |
| `VERIFY_PARSERS`        | Space-separated list of parsers to run for `verify` target.                     | (every parser except Menhir)                         |
| `PARSE_PARSERS`         | Space-separated list of parsers to run for `parse` target.                      | `menhir $(VERIFY_PARSERS)`                           |
| `BENCH_PARSERS`         | Space-separated list of parsers to run for `bench` target.                      | `$PARSE_PARSERS`                                     |

The list of supported parsers (for use with the `xxx_PARSERS` parameters) is:

  * `menhir`
  * `dypgen`
  * `pwz_nary`
  * `pwz_nary_look`
  * `pwz_binary`
  * `pwd_binary`
  * `pwd_binary_opt`
  * `pwd_nary`
  * `pwd_nary_opt`

### Included Files

We bundle version 3.4.3 of the Python standard library. This was the version of
Python used when this project originally started, and all benchmarks in the
paper were made using this version. More recent versions *should* work, but this
is the one we tested. To attempt the process with a different version, we
suggest downloading the `.tgz` file of the desired version of Python from the
Python language website, placing it in this directory, and using the `TGZ_FILE`
variable during the `make` process.

We also include a transformed version of the Python 3.4 grammar specification in
the file `./pwz_bench/utility/transformed-python-3.4.grammar`. The Python
grammar specification as-given is heavily left-recursive, which is problematic
for some parsers in this suite (such as Menhir, which is LR(1)). We manually
transformed this grammar specification to be non-left-recursive.

For producing graphs with a nicely-formatted PDF, we included the ACM's LaTeX
article class, `./graphs/acmart.cls`.

### Manual Interaction

The Makefile in this directory is used as the interface to the rest of the code.
However, it is primarily an interface for the `pwz_bench.py` script found in
this directory. The Makefile provides a more convenient manner of organizing and
manipulating parameters across various subcommands, but if you desire you can
explore the script directly. It contains full `--help`-style documentation for
each subcommand.

## Reducing Benchmark Time

The benchmarks can take a long time, due to various reasons.

Perhaps the most effective (but also "hacky") way to reduce the benchmarking
time is to reduce the total number of inputs. After running `make all`, you can
delete `.lex` files from the `$LEX_FILE_DIR` (`./lexes/` by default). The `.lex`
files contain one token on each line, and the files with more tokens tend to
take the longest, so you can use the one-line bash script `for filename in
./lexes/*.lex; do echo -e "$(wc -l $filename)\t$filename"; done | sort` to count
the number of lines in each file and output them (with the line count) in sorted
order. You could then go through this list in reverse order and remove as many
files as you feel is necessary to reduce the benchmarking time.

An alternative method is to set a maximum timeout by way of the `TIMEOUT`
parameter. This can be set when using the Makefile by doing, e.g., `TIMEOUT=3
make benchmark`, which would prevent any individual benchmark from taking more
than 3 seconds. Benchmarks which go over the time will be marked as failures and
discarded. By default, there is no timeout imposed in this manner (i.e., it is
set to -1).

Another option is to make use of the `QUOTA_FACTOR` or `MAX_QUOTA` parameters in
the Makefile. Initially, all benchmarks are capped with a quota of 1 second. Any
benchmark which takes longer than this is pushed onto a queue, which will then
be consumed in-order after the remaining benchmarks are attempted. Each time an
item is pushed onto the queue, its quota is multiplied by the `QUOTA_FACTOR`. A
large `QUOTA_FACTOR` will cause slower benchmarks to take exponentially longer
until they are given enough time to complete or they eventually hit the
`MAX_QUOTA`, at which point they will be marked as failures. The default
`QUOTA_FACTOR` is 3, and the default `MAX_QUOTA` is 1000.

## Parsing

Although not necessary for running benchmarks, the resulting ASTs of each parse
can be output to file. This was primarily developed for debugging, but can be
used to inspect parse results manually.

Assuming you have `.lex` files in `$LEX_FILE_DIR` (which can be achieved by
running `make lex` or `make all`; see above), you can produce `.ast` parse
results in the `$AST_FILE_DIR` directory by running:

```
$ make parse
```

Successful parses (i.e., parses which do not produce a shell error) are denoted
with ✅, and errors encountered (such as those due to faulty `.lex` inputs) are
marked with ❌. Errors for each parser `$parser` will also be logged to file in
`$AST_FILE_DIR/$parser-parse-errors.txt`.

The output of each (successful) parse is a `.ast` file containing an OCaml AST
of the resulting parse, formatted as a single line to reduce overhead caused by
whitespace. For a given `.lex` file `$LEX_FILE_DIR/$filename.lex`, parser
`$parser` will output its corresponding `.ast` file to
`$AST_FILE_DIR/$parser/$filename.lex.ast`.

### Verification

To spare you and your research assistants from the need to manually sift through
the resulting data and compare the outputs, we have also included a verification
target in the Makefile:

```
$ make verify
```

This uses Menhir's outputs as a ground truth. We believe this is most
reasonable, as Menhir is the most community-tested parser tested in this suite,
so we assume it is most likely to be correct if there were any discrepancy. All
other parsers' outputs are therefore compared to Menhir's. Successful
comparisons are shown with ✅, while failed comparisons will show ❌. Failed
comparisons will also log the failing files' names to
`$AST_FILE_DIR/$parser-verify-errors.txt`.
